{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nms import nms\n",
    "from progress.bar import Bar\n",
    "from PIL import Image, ImageDraw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pgm_image(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        header = f.readline().decode('ascii')\n",
    "        assert header.strip() == 'P5', \"PGM 파일이 아니거나 지원되지 않는 PGM 형식입니다.\"\n",
    "        \n",
    "        # 주석 건너뛰기\n",
    "        while True:\n",
    "            line = f.readline().decode('ascii')\n",
    "            if line[0] != '#':\n",
    "                break\n",
    "        \n",
    "        # 너비와 높이 읽기\n",
    "        width, height = [int(i) for i in line.split()]\n",
    "        max_val = int(f.readline().decode('ascii'))  # 최대 픽셀 값 (예: 255)\n",
    "        \n",
    "        # 이미지 데이터 읽기\n",
    "        img = np.fromfile(f, dtype=np.uint8).reshape((height, width))\n",
    "        \n",
    "        return img\n",
    "\n",
    "def load_all_pgms(folder_path):\n",
    "    pgm_images = []\n",
    "    \n",
    "    # 폴더 내 모든 PGM 파일 불러오기\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.pgm'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            img = load_pgm_image(file_path)\n",
    "            pgm_images.append(img)\n",
    "    \n",
    "    # 이미지 리스트를 NumPy 배열로 변환\n",
    "    pgm_images_array = np.array(pgm_images)\n",
    "    \n",
    "    return pgm_images_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train X: (6977, 19, 19)\n",
      "train y: (6977,)\n"
     ]
    }
   ],
   "source": [
    "train_face_folder_path = '/workspace/mlcnn/mitcbcl/train/face'\n",
    "train_non_face_folder_path = '/workspace/mlcnn/mitcbcl/train/non-face'\n",
    "\n",
    "train_face_images = load_all_pgms(train_face_folder_path)\n",
    "train_non_face_images = load_all_pgms(train_non_face_folder_path)\n",
    "\n",
    "train_X = np.concatenate([train_face_images, train_non_face_images], axis=0)\n",
    "train_y = np.zeros(len(train_face_images) + len(train_non_face_images))\n",
    "train_y[:len(train_face_images)] = 1\n",
    "\n",
    "print(f\"train X: {train_X.shape}\")\n",
    "print(f\"train y: {train_y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test X: (24045, 19, 19)\n",
      "test y: (24045,)\n"
     ]
    }
   ],
   "source": [
    "test_face_folder_path = '/workspace/mlcnn/mitcbcl/test/face'\n",
    "test_non_face_folder_path = '/workspace/mlcnn/mitcbcl/test/non-face'\n",
    "\n",
    "test_face_images = load_all_pgms(test_face_folder_path)\n",
    "test_non_face_images = load_all_pgms(test_non_face_folder_path)\n",
    "\n",
    "test_X = np.concatenate([test_face_images, test_non_face_images], axis=0)\n",
    "test_y = np.zeros(len(test_face_images) + len(test_non_face_images))\n",
    "test_y[:len(test_face_images)] = 1\n",
    "\n",
    "print(f\"test X: {test_X.shape}\")\n",
    "print(f\"test y: {test_y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Make Haar filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RectangleRegion:\n",
    "    def __init__(self, x, y, width, height):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "\n",
    "    def compute_region(self, ii, scale=1.0):\n",
    "        x1 = int(self.x * scale)\n",
    "        y1 = int(self.y * scale)\n",
    "        x2 = x1 + int(self.width * scale) - 1\n",
    "        y2 = y1 + int(self.height * scale) - 1\n",
    "\n",
    "        # 사각형 영역 내 적분 이미지 값 계산\n",
    "        S = int(ii[x2, y2])\n",
    "        if x1 > 0: S -= int(ii[x1-1, y2])\n",
    "        if y1 > 0: S -= int(ii[x2, y1-1])\n",
    "        if x1 > 0 and y1 > 0: S += int(ii[x1 - 1, y1 - 1])\n",
    "        return S  # 부호 없는 값으로 인한 계산 보정\n",
    "\n",
    "class HaarFeature:\n",
    "    def __init__(self, positive_regions, negative_regions):\n",
    "        self.positive_regions = positive_regions  # 흰색 영역\n",
    "        self.negative_regions = negative_regions  # 검은색 영역\n",
    "\n",
    "    def compute_value(self, ii, scale=1.0):\n",
    "        \"\"\"\n",
    "        적분 이미지에서 특징 값 계산\n",
    "        \"\"\"\n",
    "        sum_pos = sum([rect.compute_region(ii, scale) for rect in self.positive_regions])\n",
    "        sum_neg = sum([rect.compute_region(ii, scale) for rect in self.negative_regions])\n",
    "        return sum_neg - sum_pos  # 특징 값은 검은색 영역에서 흰색 영역을 뺀 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_filters(img_w, img_h, shift=1, scale_factor=1.25, min_w=4, min_h=4):\n",
    "    \"\"\"\n",
    "    Haar 특징으로 필터 생성\n",
    "\n",
    "    흰색 사각형에서 검은색 사각형 값을 뺌\n",
    "    \"\"\"\n",
    "    filters = []  # [Tuple(positive regions, negative regions),...]\n",
    "\n",
    "    # 특징 창 크기 조정\n",
    "    for w_width in range(min_w, img_w + 1):\n",
    "        for w_height in range(min_h, img_h + 1):\n",
    "\n",
    "            # 이미지 전체 순회\n",
    "            x = 0\n",
    "            while x + w_width < img_w:\n",
    "                y = 0\n",
    "                while y + w_height < img_h:\n",
    "\n",
    "                    # 가능한 Haar 영역 설정\n",
    "                    immediate = RectangleRegion(x, y, w_width, w_height)  # |X|\n",
    "                    right = RectangleRegion(x + w_width, y, w_width, w_height)  # | |X|\n",
    "                    right_2 = RectangleRegion(x + w_width * 2, y, w_width, w_height)  # | | |X|\n",
    "                    bottom = RectangleRegion(x, y + w_height, w_width, w_height)  # | |/|X|\n",
    "                    bottom_right = RectangleRegion(x + w_width, y + w_height, w_width, w_height)  # | |/| |X|\n",
    "\n",
    "                    # [Haar] 2개의 사각형 *********\n",
    "                    # 수평 (흰색-검은색)\n",
    "                    if x + w_width * 2 < img_w:\n",
    "                        filters.append(HaarFeature([immediate], [right]))\n",
    "                    # 수직 (흰색-검은색)\n",
    "                    if y + w_height * 2 < img_h:\n",
    "                        filters.append(HaarFeature([bottom], [immediate]))\n",
    "\n",
    "                    # [Haar] 3개의 사각형 *********\n",
    "                    # 수평 (흰색-검은색-흰색)\n",
    "                    if x + w_width * 3 < img_w:\n",
    "                        filters.append(HaarFeature([immediate, right_2], [right]))\n",
    "\n",
    "                    # [Haar] 4개의 사각형 *********\n",
    "                    if x + w_width * 2 < img_w and y + w_height * 2 < img_h:\n",
    "                        filters.append(HaarFeature([immediate, bottom_right], [bottom, right]))\n",
    "\n",
    "                    y += shift\n",
    "                x += shift\n",
    "    return filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Get features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_filters(X_ii, filters):\n",
    "    \"\"\"\n",
    "    학습 데이터(적분 이미지)에 필터 적용\n",
    "    \"\"\"\n",
    "\n",
    "    X = np.zeros((len(filters), len(X_ii)), dtype=np.int32)\n",
    "\n",
    "    bar = Bar('필터 처리 중', max=len(filters), suffix='%(percent)d%% - %(elapsed_td)s - %(eta_td)s')\n",
    "    for j, feature in bar.iter(enumerate(filters)):\n",
    "        # 필터 'j'의 값을 학습 이미지들에 대해 계산 (각 분류기의 입력으로 사용)\n",
    "        X[j] = list(map(lambda ii: feature.compute_value(ii), X_ii))\n",
    "    bar.finish()\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integral_image(img):\n",
    "    \"\"\"\n",
    "    합산 영역 테이블의 최적화 버전\n",
    "    ii(-1, y) = 0  # 좌측 경계 처리\n",
    "    s(x, -1) = 0  # 상단 경계 처리\n",
    "    s(x, y) = s(x, y-1) + i(x, y)  # 열 X의 y 레벨까지의 합\n",
    "    ii(x, y) = ii(x-1, y) + s(x, y)  # (X-1, Y) 위치의 적분 이미지 + 열 X의 y 레벨까지의 합\n",
    "    \"\"\"\n",
    "    h, w = img.shape\n",
    "\n",
    "    s = np.zeros(img.shape, dtype=np.uint32)  # 각 열의 누적 합을 저장할 배열\n",
    "    ii = np.zeros(img.shape, dtype=np.uint32)  # 적분 이미지 배열\n",
    "\n",
    "    for x in range(0, w):\n",
    "        for y in range(0, h):\n",
    "            # 현재 픽셀을 포함한 y까지의 열 합 계산\n",
    "            s[y][x] = s[y - 1][x] + img[y][x] if y - 1 >= 0 else img[y][x]\n",
    "            # 적분 이미지 계산 (이전 열의 적분값과 현재 열의 누적 합을 더함)\n",
    "            ii[y][x] = ii[y][x - 1] + s[y][x] if x - 1 >= 0 else s[y][x]\n",
    "    \n",
    "    return ii"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeakClassifier:\n",
    "\n",
    "    def __init__(self, haar_feature=None, threshold=None, polarity=None):\n",
    "        # 약한 분류기 초기화\n",
    "        self.haar_feature = haar_feature\n",
    "        self.threshold = threshold\n",
    "        self.polarity = polarity\n",
    "\n",
    "    def classify(self, ii, scale=1.0):\n",
    "        \"\"\"\n",
    "        적분 이미지 'ii'와 스케일이 주어졌을 때 분류를 수행 (=추론)\n",
    "        \"\"\"\n",
    "        # 주어진 적분 이미지에서 Haar feature 값 계산\n",
    "        feature_value = self.haar_feature.compute_value(ii, scale)\n",
    "        # polarity와 임계값을 사용하여 분류\n",
    "        return 1 if self.polarity * feature_value < self.polarity * self.threshold * (scale**2) else 0\n",
    "\n",
    "    def classify_f(self, feature_value):\n",
    "        \"\"\"\n",
    "        주어진 feature 값 또는 배열을 사용하여 분류 (=학습)\n",
    "        \"\"\"\n",
    "        # polarity와 feature 값 비교 후 분류\n",
    "        a = self.polarity * feature_value\n",
    "        b = self.polarity * self.threshold\n",
    "        return np.less(a, b).astype(int)\n",
    "\n",
    "    def train(self, X, y, weights, total_pos_weights=None, total_neg_weights=None):\n",
    "        # 긍정/부정 가중치 합 계산 (주어지지 않은 경우)\n",
    "        if not total_pos_weights:\n",
    "            total_pos_weights = np.sum(weights[np.where(y == 1)])\n",
    "        if not total_neg_weights:\n",
    "            total_neg_weights = np.sum(weights[np.where(y == 0)])\n",
    "\n",
    "        # feature 값 기준으로 정렬\n",
    "        sorted_features = sorted(zip(weights, X, y), key=lambda a: a[1])\n",
    "\n",
    "        pos_seen, neg_seen = 0, 0\n",
    "        sum_pos_weights, sum_neg_weights = 0, 0\n",
    "        min_error, best_feature, best_threshold, best_polarity = float('inf'), None, None, None\n",
    "\n",
    "        for w, f, label in sorted_features:\n",
    "            # 오차 계산: 긍정/부정 가중치 중 작은 값 선택\n",
    "            error = min(\n",
    "                sum_neg_weights + (total_pos_weights - sum_pos_weights),\n",
    "                sum_pos_weights + (total_neg_weights - sum_neg_weights)\n",
    "            )\n",
    "\n",
    "            # 최소 오차인 경우 임계값과 polarity 업데이트\n",
    "            if error < min_error:\n",
    "                min_error = error\n",
    "                self.threshold = f\n",
    "                self.polarity = 1 if pos_seen > neg_seen else -1\n",
    "\n",
    "            # 레이블에 따라 가중치와 카운트 업데이트\n",
    "            if label == 1:\n",
    "                pos_seen += 1\n",
    "                sum_pos_weights += w\n",
    "            else:\n",
    "                neg_seen += 1\n",
    "                sum_neg_weights += w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBoost:\n",
    "    \"\"\"\n",
    "    분류 오류가 높은 샘플에 대해 더 높은 가중치를 부여함으로써 해당 샘플이 더 중요하게 다뤄지도록 만듭니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_estimators=10):\n",
    "        # AdaBoost 초기화: n_estimators는 약한 분류기의 수\n",
    "        self.n_estimators = n_estimators\n",
    "        self.alphas = []  # 약한 분류기 가중치\n",
    "        self.clfs = []  # 약한 분류기 리스트\n",
    "\n",
    "    def train(self, X, y, features, X_ii):\n",
    "        # 긍정, 부정 샘플 수 계산\n",
    "        pos_num = np.sum(y)\n",
    "        neg_num = len(y) - pos_num\n",
    "        weights = np.zeros(len(y), dtype=np.float32)\n",
    "\n",
    "        # 초기 가중치 설정\n",
    "        for i in range(len(y)):\n",
    "            if y[i] == 1:  # 긍정 샘플\n",
    "                weights[i] = 1.0 / (pos_num * 2.0)\n",
    "            else:  # 부정 샘플\n",
    "                weights[i] = 1.0 / (neg_num * 2.0)\n",
    "\n",
    "        print(\"Training...\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        # 각 약한 분류기를 학습\n",
    "        for t in range(self.n_estimators):\n",
    "            print(f\"Training {t + 1} classifiers out of {self.n_estimators}\")\n",
    "\n",
    "            # 가중치 정규화\n",
    "            w_sum = np.sum(weights)\n",
    "            if w_sum == 0.0:\n",
    "                print(\"[WARNING] EARLY STOP. WEIGHTS ARE ZERO.\")\n",
    "                break\n",
    "            weights = weights / w_sum\n",
    "\n",
    "            # 약한 분류기 학습\n",
    "            print(\"Training weak classifiers...\")\n",
    "            weak_classifiers = self.train_estimators(X, y, weights, features)\n",
    "\n",
    "            # 가장 낮은 오차를 가진 약한 분류기 선택\n",
    "            print(\"Selecting best weak classifiers...\")\n",
    "            clf, error, incorrectness = self.select_best(weak_classifiers, X, y, weights)\n",
    "\n",
    "            if error <= 0.5:\n",
    "                # alpha와 beta 계산\n",
    "                beta = error / (1.0 - error)\n",
    "                alpha = math.log(1.0 / (beta + 1e-18))\n",
    "\n",
    "                # 가중치 업데이트\n",
    "                weights = np.multiply(weights, beta ** (1 - incorrectness))\n",
    "\n",
    "                # 최종 분류기 및 가중치 저장\n",
    "                self.alphas.append(alpha)\n",
    "                self.clfs.append(clf)\n",
    "            else:\n",
    "                print(error)\n",
    "                print(\"WHAT THE FUCK!????\")\n",
    "\n",
    "        print(f\"<== Training completed. Num. classifiers: {self.n_estimators}\")\n",
    "\n",
    "    def train_estimators(self, X, y, weights, features):\n",
    "        # 약한 분류기 학습\n",
    "        weak_clfs = []\n",
    "        total_pos_weights, total_neg_weights = 0, 0\n",
    "\n",
    "        for w, label in zip(weights, y):\n",
    "            if label == 1:\n",
    "                total_pos_weights += w\n",
    "            else:\n",
    "                total_neg_weights += w\n",
    "\n",
    "        for i in range(len(X)):\n",
    "            # 각 feature에 대해 약한 분류기 학습\n",
    "            clf = WeakClassifier(haar_feature=features[i])\n",
    "            clf.train(X[i], y, weights, total_pos_weights, total_neg_weights)\n",
    "            weak_clfs.append(clf)\n",
    "\n",
    "        return weak_clfs\n",
    "\n",
    "    def select_best(self, weak_clfs, X, y, weights):\n",
    "        # 최적의 약한 분류기를 선택\n",
    "        best_clf, min_error, best_accuracy = None, float('inf'), None\n",
    "\n",
    "        for i, clf in enumerate(weak_clfs):\n",
    "            # 오차 계산\n",
    "            incorrectness = np.abs(clf.classify_f(X[i]) - y)\n",
    "            error = float(np.sum(np.multiply(incorrectness, weights))) / len(incorrectness)\n",
    "\n",
    "            # 최소 오차를 가진 분류기 선택\n",
    "            if error < min_error:\n",
    "                best_clf, min_error, best_accuracy = clf, error, incorrectness\n",
    "\n",
    "        return best_clf, min_error, best_accuracy\n",
    "\n",
    "    def classify(self, X, scale=1.0):\n",
    "        # 최종 분류 수행: 여러 약한 분류기의 가중치를 반영한 결과 반환\n",
    "        total = sum(list(map(lambda x: x[0] * x[1].classify(X, scale), zip(self.alphas, self.clfs))))\n",
    "        return 1 if total >= 0.5 * sum(self.alphas) else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViolaJones:\n",
    "\n",
    "    def __init__(self, layers, features_path=None):\n",
    "        assert isinstance(layers, list)\n",
    "        self.layers = layers  # 약한 분류기의 개수(T)를 포함한 리스트\n",
    "        self.clfs = []\n",
    "        self.base_width, self.base_height = 19, 19  # 학습 데이터셋 이미지의 기본 크기\n",
    "        self.base_scale, self.shift = 1, 2\n",
    "        print(f\"self.base_scale: {self.base_scale}\")\n",
    "        self.features_path = features_path  # 특징 저장 경로\n",
    "\n",
    "    def train(self, X, y):\n",
    "        \"\"\"\n",
    "        N개의 Viola-Jones 분류기(AdaBoost)를 점점 복잡하게 학습시킴.\n",
    "        첫 번째 분류기 이후에는, 각 분류기는 양성 예시와 이전 분류기의 \n",
    "        오탐(false positive) 예시에 대해 학습함.\n",
    "        \"\"\"\n",
    "        print(\"데이터 준비 중...\")\n",
    "\n",
    "        # 학습 데이터 준비\n",
    "        pos_num = np.sum(y)  # 양성 예시 개수\n",
    "        neg_num = len(y) - pos_num  # 음성 예시 개수\n",
    "        img_h, img_w = X[0].shape  # 모든 학습 이미지는 동일한 크기를 가짐\n",
    "\n",
    "        # 양성 및 음성 샘플 분리\n",
    "        pos_indices = np.where(y == 1)[0]\n",
    "        neg_indices = np.where(y == 0)[0]\n",
    "\n",
    "        # 데이터 정보 출력\n",
    "        print(\"입력 데이터 요약:\")\n",
    "        print(\"\\t- 얼굴 개수: {:,} ({:.2f}%)\".format(int(pos_num), 100.0 * pos_num / (pos_num + neg_num)))\n",
    "        print(\"\\t- 얼굴이 아닌 이미지 개수: {:,} ({:.2f}%)\".format(int(neg_num), 100.0 * neg_num / (pos_num + neg_num)))\n",
    "        print(\"\\t- 전체 샘플 수: {:,}\".format(int(pos_num + neg_num)))\n",
    "        print(\"\\t- 이미지 크기 (가로x세로): {}x{}\".format(img_w, img_h))\n",
    "\n",
    "        # 가중치 초기화 및 적분 이미지 생성\n",
    "        print(\"적분 이미지 생성 중...\")\n",
    "        start_time = time.time()\n",
    "        X_ii = np.array(list(map(lambda x: integral_image(x), X)), dtype=np.uint32)\n",
    "        print(\"\\t- 적분 이미지 개수: {:,}\".format(len(X_ii)))\n",
    "\n",
    "        # 필터 생성 및 적용\n",
    "        print(\"필터 생성 중...\")\n",
    "        start_time = time.time()\n",
    "        filters = build_filters(img_w, img_h)  # 모든 이미지에 동일한 필터 사용\n",
    "        print(\"\\t- 생성된 필터 수: {:,}\".format(len(filters)))\n",
    "\n",
    "        print(\"필터 적용 중...\")\n",
    "        start_time = time.time()\n",
    "        X_f = self.__load_feature_dataset()  # 저장된 특징 데이터셋 불러오기 (존재하는 경우)\n",
    "        if X_f is None:\n",
    "            X_f = apply_filters(X_ii, filters)\n",
    "\n",
    "            if self.features_path:  # 특징 저장\n",
    "                np.save(self.features_path + \"xf\" + \".npy\", X_f)\n",
    "                print(\"추출된 특징 파일 저장 완료!\")\n",
    "        print(\"\\t- 추출된 특징 수: {:,}\".format(len(X_f) * len(filters)))\n",
    "\n",
    "        # Cascade 방식의 Viola-Jones 분류기 학습 (AdaBoost)\n",
    "        for i, t in enumerate(self.layers):\n",
    "            print(\"[CascadeClassifier] {}층 중 {}번째 층 학습 중\".format(len(self.layers), i+1))\n",
    "            if len(neg_indices) == 0:\n",
    "                print('조기 종료: 모든 샘플이 올바르게 분류되었습니다.')\n",
    "                break\n",
    "\n",
    "            # 샘플 인덱스를 섞음 (=음성과 양성을 무작위로 섞음)\n",
    "            tr_idxs = np.concatenate([pos_indices, neg_indices])\n",
    "            np.random.shuffle(tr_idxs)\n",
    "\n",
    "            # Viola-Jones (AdaBoost) 학습\n",
    "            clf = AdaBoost(n_estimators=t)\n",
    "            clf.train(X_f[:, tr_idxs], y[tr_idxs], filters, X_ii[tr_idxs])\n",
    "            self.clfs.append(clf)\n",
    "\n",
    "            # 얼굴이 아닌 이미지 중 얼굴로 분류된 false positive 탐지\n",
    "            false_positives = []\n",
    "            for neg_idx in neg_indices:\n",
    "                if self.classify(X[neg_idx]) == 1:\n",
    "                    false_positives.append(neg_idx)\n",
    "            neg_indices = np.array(false_positives)\n",
    "\n",
    "    def classify(self, image, scale=1.0):\n",
    "        \"\"\"\n",
    "        얼굴이 아닌 것으로 판정되면 즉시 종료.\n",
    "        얼굴이 아닐 경우 계속해서 검사 진행.\n",
    "        \"\"\"\n",
    "        return self.classify_ii(integral_image(image), scale)\n",
    "\n",
    "    def classify_ii(self, ii, scale=1.0):\n",
    "        \"\"\"\n",
    "        얼굴이 아닌 것으로 판정되면 즉시 종료.\n",
    "        얼굴이 아닐 경우 계속해서 검사 진행.\n",
    "        \"\"\"\n",
    "        for clf in self.clfs:  # ViolaJones 분류기\n",
    "            if clf.classify(ii, scale) == 0:\n",
    "                return 0\n",
    "        return 1\n",
    "\n",
    "    def find_faces(self, pil_image):\n",
    "        \"\"\"\n",
    "        PIL 이미지에서 얼굴을 탐지\n",
    "        \"\"\"\n",
    "        \n",
    "        w, h, s = (self.base_width, self.base_height, self.base_scale)\n",
    "        regions = []\n",
    "\n",
    "        # 이미지 전처리 (흑백 변환)\n",
    "        pil_image = pil_image.convert('L')\n",
    "        image = np.array(pil_image)\n",
    "        img_h, img_w = image.shape\n",
    "        # print(f\"w: {w}, h: {h}, s: {s}, img_w: {img_w}, img_h: {img_h}\")\n",
    "        # print(f\"w * s: {w * s}\")\n",
    "        # print(f\"w * h: {w * h}\")\n",
    "\n",
    "        # 적분 이미지 계산\n",
    "        ii = integral_image(image)\n",
    "\n",
    "        # 슬라이딩 윈도우 탐색\n",
    "        # 탐색 영역의 크기가 이미지보다 작아야 함\n",
    "        counter = 0\n",
    "        while int(w * s) < img_w and int(h * s) < img_h:\n",
    "            print(f\"while break\")\n",
    "\n",
    "            # 탐색 영역이 이미지 범위 안에 있어야 함\n",
    "            for y1 in np.arange(0, int(img_h) - int(h * s), self.shift):\n",
    "                print(f\"for1 break\")\n",
    "                for x1 in np.arange(0, int(img_w) - int(w * s), self.shift):\n",
    "                    print(f\"for2 break\")\n",
    "                    print(f\"y1: {y1}\")\n",
    "                    print(F\"x1: {x1}\")\n",
    "\n",
    "                    y1, x1 = int(y1), int(x1)\n",
    "                    y2, x2 = y1 + int(h * s), x1 + int(w * s)\n",
    "                    cropped_img = ii[y1:y2, x1:x2]\n",
    "\n",
    "                    if self.classify_ii(cropped_img, scale=s):  # CascadeClassifier를 통한 분류\n",
    "                        regions.append((x1, y1, x2, y2))\n",
    "\n",
    "                    counter += 1\n",
    "                    print(\"분석된 크롭 수: {}\".format(counter))\n",
    "\n",
    "            # 탐색 윈도우 크기 확대\n",
    "            w *= s\n",
    "            h *= s\n",
    "\n",
    "        return regions\n",
    "\n",
    "    def save(self, filename):\n",
    "        with open(filename + \".pkl\", 'wb') as f:\n",
    "            pickle.dump(self, f)\n",
    "\n",
    "    @staticmethod\n",
    "    def load(filename):\n",
    "        with open(filename, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "    def __load_feature_dataset(self):\n",
    "        X_f = None\n",
    "        # 사전 계산된 특징 데이터셋 로드\n",
    "        try:\n",
    "            if self.features_path:\n",
    "                X_f = np.load(self.features_path + \"xf\" + \".npy\")\n",
    "                print(\"사전 계산된 데이터셋 로드 완료!\")\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "        return X_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Viola-Jones 훈련 중...\n",
      "self.base_scale: 1\n",
      "데이터 준비 중...\n",
      "입력 데이터 요약:\n",
      "\t- 얼굴 개수: 2,429 (34.81%)\n",
      "\t- 얼굴이 아닌 이미지 개수: 4,548 (65.19%)\n",
      "\t- 전체 샘플 수: 6,977\n",
      "\t- 이미지 크기 (가로x세로): 19x19\n",
      "적분 이미지 생성 중...\n",
      "\t- 적분 이미지 개수: 6,977\n",
      "필터 생성 중...\n",
      "\t- 생성된 필터 수: 11,376\n",
      "필터 적용 중...\n",
      "사전 계산된 데이터셋 로드 완료!\n",
      "\t- 추출된 특징 수: 129,413,376\n",
      "[CascadeClassifier] 4층 중 1번째 층 학습 중\n",
      "Training...\n",
      "Training 1 classifiers out of 1\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "<== Training completed. Num. classifiers: 1\n",
      "[CascadeClassifier] 4층 중 2번째 층 학습 중\n",
      "Training...\n",
      "Training 1 classifiers out of 10\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 2 classifiers out of 10\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 3 classifiers out of 10\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 4 classifiers out of 10\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 5 classifiers out of 10\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 6 classifiers out of 10\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 7 classifiers out of 10\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 8 classifiers out of 10\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 9 classifiers out of 10\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 10 classifiers out of 10\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "<== Training completed. Num. classifiers: 10\n",
      "[CascadeClassifier] 4층 중 3번째 층 학습 중\n",
      "Training...\n",
      "Training 1 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 2 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 3 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 4 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 5 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 6 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 7 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 8 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 9 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 10 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 11 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 12 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 13 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 14 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 15 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 16 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 17 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 18 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 19 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 20 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 21 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 22 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 23 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 24 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 25 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 26 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 27 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 28 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 29 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 30 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 31 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 32 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 33 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 34 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 35 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 36 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 37 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 38 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 39 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 40 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 41 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 42 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 43 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 44 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 45 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 46 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 47 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 48 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 49 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 50 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "<== Training completed. Num. classifiers: 50\n",
      "[CascadeClassifier] 4층 중 4번째 층 학습 중\n",
      "Training...\n",
      "Training 1 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 2 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 3 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 4 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 5 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 6 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 7 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 8 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 9 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 10 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 11 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 12 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 13 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 14 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 15 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 16 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 17 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 18 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 19 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 20 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 21 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 22 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 23 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 24 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 25 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 26 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 27 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 28 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 29 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 30 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 31 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 32 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 33 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 34 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 35 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 36 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 37 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 38 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 39 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 40 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 41 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 42 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 43 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 44 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 45 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 46 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 47 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 48 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 49 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 50 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 51 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 52 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 53 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 54 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 55 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 56 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 57 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 58 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 59 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 60 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 61 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 62 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 63 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 64 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 65 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 66 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 67 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 68 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 69 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 70 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 71 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 72 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 73 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 74 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 75 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 76 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 77 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 78 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 79 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 80 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 81 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 82 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 83 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 84 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 85 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 86 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 87 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 88 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 89 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 90 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 91 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 92 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 93 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 94 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 95 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 96 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 97 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 98 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 99 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 100 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "<== Training completed. Num. classifiers: 100\n",
      "훈련 완료!\n",
      "\n",
      "가중치 저장 중...\n",
      "가중치 저장 완료!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nViola-Jones 훈련 중...\")\n",
    "features_path = '/workspace/HDD/save/'\n",
    "clf = ViolaJones(layers=[1, 10, 50, 100], features_path=features_path)\n",
    "clf.train(train_X, train_y)  # X_f (선택 사항, 훈련 속도 향상용)\n",
    "print(\"훈련 완료!\")\n",
    "\n",
    "# 가중치 저장\n",
    "print(\"\\n가중치 저장 중...\")\n",
    "clf.save(features_path + 'cvj_weights_' + str(int(time.time())))\n",
    "print(\"가중치 저장 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(clf, X, y, show_samples=False):\n",
    "    metrics = {}\n",
    "    true_positive, true_negative = 0, 0  # 올바른 예측 수\n",
    "    false_positive, false_negative = 0, 0  # 잘못된 예측 수\n",
    "\n",
    "    for i in range(len(y)):\n",
    "        prediction = clf.classify(X[i])\n",
    "        if prediction == y[i]:  # 올바른 예측\n",
    "            if prediction == 1:  # 얼굴\n",
    "                true_positive += 1\n",
    "            else:  # 비얼굴\n",
    "                true_negative += 1\n",
    "        else:  # 잘못된 예측\n",
    "            # if show_samples: show_sample(X[i], y[i], prediction)\n",
    "\n",
    "            if prediction == 1:  # 얼굴\n",
    "                false_positive += 1\n",
    "            else:  # 비얼굴\n",
    "                false_negative += 1\n",
    "\n",
    "    # 지표 계산\n",
    "    metrics['true_positive'] = true_positive\n",
    "    metrics['true_negative'] = true_negative\n",
    "    metrics['false_positive'] = false_positive\n",
    "    metrics['false_negative'] = false_negative\n",
    "\n",
    "    metrics['accuracy'] = (true_positive + true_negative) / (true_positive + false_negative + true_negative + false_positive)  # 정확도\n",
    "    metrics['precision'] = true_positive / (true_positive + false_positive)  # 정밀도\n",
    "    metrics['recall'] = true_positive / (true_positive + false_negative)  # 재현율 또는 민감도\n",
    "    metrics['specifity'] = true_negative / (true_negative + false_positive)  # 특이도\n",
    "    metrics['f1'] = (2.0 * metrics['precision'] * metrics['recall']) / (metrics['precision'] + metrics['recall'])  # F1 점수\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_path = \"/workspace/HDD/save/cvj_weights_1729225775.pkl\"\n",
    "\n",
    "clf = ViolaJones.load(weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "name = \"test\"\n",
    "print(\"\\n평가 중...\")\n",
    "metrics = evaluate(clf, test_X, test_y, show_samples=False)\n",
    " \n",
    "print(\"지표: [{}]\".format(name))\n",
    "counter = 0\n",
    "for k, v in metrics.items():\n",
    "    counter += 1\n",
    "    if counter <= 4:\n",
    "        print(\"\\t- {}: {:,}\".format(k, v))\n",
    "    else:\n",
    "        print(\"\\t- {}: {:.3f}\".format(k, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bounding_boxes(pil_image, regions, color=\"green\", thickness=3):\n",
    "    # 이미지를 준비합니다.\n",
    "    source_img = pil_image.convert(\"RGBA\")\n",
    "    draw = ImageDraw.Draw(source_img)\n",
    "    for rect in regions:\n",
    "        draw.rectangle(tuple(rect), outline=color, width=thickness)\n",
    "    return source_img\n",
    "\n",
    "\n",
    "def find_faces(weight_path, folder_path):\n",
    "    # 분류기 가중치를 로드합니다.\n",
    "    clf = ViolaJones.load(weight_path)\n",
    "\n",
    "    # 얼굴의 영역을 찾습니다.\n",
    "    numpy_img = load_all_pgms(folder_path)\n",
    "    for idx, img in enumerate(numpy_img):\n",
    "        pil_img = Image.fromarray(img)\n",
    "        regions = clf.find_faces(pil_img)\n",
    "\n",
    "        if regions:\n",
    "            # 바운딩 박스를 그립니다.\n",
    "            # TODO: 비최대 억제(Non-maximum suppression) 검토 (자체 구현 수정)\n",
    "            scores = [1.0] * len(regions)  #np.ones(len(regions))\n",
    "            indicies = nms.boxes(regions, scores)\n",
    "            regions = np.array(regions)\n",
    "            print(f\"scores: {scores}\")\n",
    "            print(f\"regions: {regions}\")\n",
    "\n",
    "            drawn_img = draw_bounding_boxes(pil_img, list(regions[indicies]), thickness=1)\n",
    "            # drawn_img = draw_bounding_boxes(pil_img, list(regions), thickness=1)\n",
    "\n",
    "            # 이미지를 표시합니다.\n",
    "            plt.imshow(drawn_img)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_path = \"/workspace/HDD/save/cvj_weights_1729225775.pkl\"\n",
    "face_path = \"/workspace/mlcnn/mitcbcl/train/face\"\n",
    "\n",
    "find_faces(weight_path, face_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
