{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import pickle\n",
    "import numpy as np\n",
    "from progress.bar import Bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_folder_path = '/workspace/mlcnn/mitcbcl/train/face'\n",
    "non_face_folder_path = '/workspace/mlcnn/mitcbcl/train/non-face'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pgm_image(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        header = f.readline().decode('ascii')\n",
    "        assert header.strip() == 'P5', \"Not a PGM file or unsupported PGM format.\"\n",
    "        \n",
    "        # Skip comments\n",
    "        while True:\n",
    "            line = f.readline().decode('ascii')\n",
    "            if line[0] != '#':\n",
    "                break\n",
    "        \n",
    "        # Read width, height\n",
    "        width, height = [int(i) for i in line.split()]\n",
    "        max_val = int(f.readline().decode('ascii'))  # Max pixel value (e.g. 255)\n",
    "        \n",
    "        # Read image data\n",
    "        img = np.fromfile(f, dtype=np.uint8).reshape((height, width))\n",
    "        \n",
    "        return img\n",
    "\n",
    "def load_all_pgms(folder_path):\n",
    "    pgm_images = []\n",
    "    \n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.pgm'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            img = load_pgm_image(file_path)\n",
    "            pgm_images.append(img)\n",
    "    \n",
    "    # Convert list of images to a single NumPy array\n",
    "    pgm_images_array = np.array(pgm_images)\n",
    "    \n",
    "    return pgm_images_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "face_images: (2429, 19, 19)\n",
      "non_face_images: (4548, 19, 19)\n"
     ]
    }
   ],
   "source": [
    "face_images = load_all_pgms(face_folder_path)\n",
    "non_face_images = load_all_pgms(non_face_folder_path)\n",
    "\n",
    "print(f\"face_images: {face_images.shape}\")\n",
    "print(f\"non_face_images: {non_face_images.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (6977, 19, 19)\n",
      "y: (6977,)\n"
     ]
    }
   ],
   "source": [
    "X = np.concatenate([face_images, non_face_images], axis=0)\n",
    "y = np.zeros(len(face_images) + len(non_face_images))\n",
    "y[:len(face_images)] = 1\n",
    "\n",
    "print(f\"X: {X.shape}\")\n",
    "print(f\"y: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RectangleRegion:\n",
    "    def __init__(self, x, y, width, height):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "\n",
    "    def compute_region(self, ii, scale=1.0):\n",
    "        x1 = int(self.x * scale)\n",
    "        y1 = int(self.y * scale)\n",
    "        x2 = x1 + int(self.width * scale) - 1\n",
    "        y2 = y1 + int(self.height * scale) - 1\n",
    "\n",
    "        S = int(ii[x2, y2])\n",
    "        if x1 > 0: S -= int(ii[x1-1, y2])\n",
    "        if y1 > 0: S -= int(ii[x2, y1-1])\n",
    "        if x1 > 0 and y1 > 0: S += int(ii[x1 - 1, y1 - 1])\n",
    "        return S  # Due to the use of substraction with unsigned values\n",
    "\n",
    "\n",
    "class HaarFeature:\n",
    "    def __init__(self, positive_regions, negative_regions):\n",
    "        self.positive_regions = positive_regions  # White\n",
    "        self.negative_regions = negative_regions  # Black\n",
    "\n",
    "    def compute_value(self, ii, scale=1.0):\n",
    "        \"\"\"\n",
    "        Compute the value of a feature(x,y,w,h) at the integral image\n",
    "        \"\"\"\n",
    "\n",
    "        sum_pos = sum([rect.compute_region(ii, scale) for rect in self.positive_regions])\n",
    "        sum_neg = sum([rect.compute_region(ii, scale) for rect in self.negative_regions])\n",
    "        return sum_neg - sum_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_filters(img_w, img_h, shift=1, scale_factor=1.25, min_w=4, min_h=4):\n",
    "    \"\"\"\n",
    "    Generate values from Haar features\n",
    "\n",
    "    White rectangles substract from black ones\n",
    "    \"\"\"\n",
    "    features = []  # [Tuple(positive regions, negative regions),...]\n",
    "\n",
    "    # Scale feature window\n",
    "    for w_width in range(min_w, img_w + 1):\n",
    "        for w_height in range(min_h, img_h + 1):\n",
    "\n",
    "            # Walk through all the image\n",
    "            x = 0\n",
    "            while x + w_width < img_w:\n",
    "                y = 0\n",
    "                while y + w_height < img_h:\n",
    "\n",
    "                    # Possible Haar regions\n",
    "                    immediate = RectangleRegion(x, y, w_width, w_height)  # |X|\n",
    "                    right = RectangleRegion(x + w_width, y, w_width, w_height)  # | |X|\n",
    "                    right_2 = RectangleRegion(x + w_width * 2, y, w_width, w_height)  # | | |X|\n",
    "                    bottom = RectangleRegion(x, y + w_height, w_width, w_height)  # | |/|X|\n",
    "                    #bottom_2 = RectangleRegion(x, y + w_height * 2, w_width, w_height)  # | |/| |/|X|\n",
    "                    bottom_right = RectangleRegion(x + w_width, y + w_height, w_width, w_height)  # | |/| |X|\n",
    "\n",
    "                    # [Haar] 2 rectagles *********\n",
    "                    # Horizontal (w-b)\n",
    "                    if x + w_width * 2 < img_w:\n",
    "                        features.append(HaarFeature([immediate], [right]))\n",
    "                    # Vertical (w-b)\n",
    "                    if y + w_height * 2 < img_h:\n",
    "                        features.append(HaarFeature([bottom], [immediate]))\n",
    "\n",
    "                    # [Haar] 3 rectagles *********\n",
    "                    # Horizontal (w-b-w)\n",
    "                    if x + w_width * 3 < img_w:\n",
    "                        features.append(HaarFeature([immediate, right_2], [right]))\n",
    "                    # # Vertical (w-b-w)\n",
    "                    # if y + w_height * 3 < img_h:\n",
    "                    #     features.append(HaarFeature([immediate, bottom_2], [bottom]))\n",
    "\n",
    "                    # [Haar] 4 rectagles *********\n",
    "                    if x + w_width * 2 < img_w and y + w_height * 2 < img_h:\n",
    "                        features.append(HaarFeature([immediate, bottom_right], [bottom, right]))\n",
    "\n",
    "                    y += shift\n",
    "                x += shift\n",
    "    return features  # np.array(features)\n",
    "\n",
    "\n",
    "def apply_filters(X_ii, filters):\n",
    "    \"\"\"\n",
    "    Apply build filters (regions) to all the training data (integral images)\n",
    "    \"\"\"\n",
    "\n",
    "    X = np.zeros((len(filters), len(X_ii)), dtype=np.int32)\n",
    "    # 'y' will be kept as it is => f0=([...], y); f1=([...], y),...\n",
    "\n",
    "    bar = Bar('Processing filters', max=len(filters), suffix='%(percent)d%% - %(elapsed_td)s - %(eta_td)s')\n",
    "    for j, feature in bar.iter(enumerate(filters)):\n",
    "    # for j, feature in enumerate(filters):\n",
    "    #     if (j + 1) % 1000 == 0 and j != 0:\n",
    "    #         print(\"Applying filters... ({}/{})\".format(j + 1, len(filters)))\n",
    "\n",
    "        # Compute the value of feature 'j' for each image in the training set (Input of the classifier_j)\n",
    "        X[j] = list(map(lambda ii: feature.compute_value(ii), X_ii))\n",
    "    bar.finish()\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integral_image(img):\n",
    "    \"\"\"\n",
    "    Optimized version of Summed-area table\n",
    "    ii(-1, y) = 0\n",
    "    s(x, -1) = 0\n",
    "    s(x, y) = s(x, y-1) + i(x, y)  # Sum of column X at level Y\n",
    "    ii(x, y) = ii(x-1, y) + s(x, y)  # II at (X-1,Y) + Column X at Y\n",
    "    \"\"\"\n",
    "    h, w = img.shape\n",
    "\n",
    "    s = np.zeros(img.shape, dtype=np.uint32)\n",
    "    ii = np.zeros(img.shape, dtype=np.uint32)\n",
    "\n",
    "    for x in range(0, w):\n",
    "        for y in range(0, h):\n",
    "            s[y][x] = s[y - 1][x] + img[y][x] if y - 1 >= 0 else img[y][x]\n",
    "            ii[y][x] = ii[y][x - 1] + s[y][x] if x - 1 >= 0 else s[y][x]\n",
    "    return ii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeakClassifier:\n",
    "\n",
    "    def __init__(self, haar_feature=None, threshold=None, polarity=None):\n",
    "        # 약한 분류기 초기화\n",
    "        self.haar_feature = haar_feature\n",
    "        self.threshold = threshold\n",
    "        self.polarity = polarity\n",
    "\n",
    "    def classify(self, ii, scale=1.0):\n",
    "        \"\"\"\n",
    "        적분 이미지 'ii'와 스케일이 주어졌을 때 분류를 수행\n",
    "        \"\"\"\n",
    "        # 주어진 적분 이미지에서 Haar feature 값 계산\n",
    "        feature_value = self.haar_feature.compute_value(ii, scale)\n",
    "        # polarity와 임계값을 사용하여 분류\n",
    "        return 1 if self.polarity * feature_value < self.polarity * self.threshold * (scale**2) else 0\n",
    "\n",
    "    def classify_f(self, feature_value):\n",
    "        \"\"\"\n",
    "        주어진 feature 값 또는 배열을 사용하여 분류\n",
    "        \"\"\"\n",
    "        # polarity와 feature 값 비교 후 분류\n",
    "        a = self.polarity * feature_value\n",
    "        b = self.polarity * self.threshold\n",
    "        return np.less(a, b).astype(int)\n",
    "\n",
    "    def train(self, X, y, weights, total_pos_weights=None, total_neg_weights=None):\n",
    "        # 긍정/부정 가중치 합 계산 (주어지지 않은 경우)\n",
    "        if not total_pos_weights:\n",
    "            total_pos_weights = np.sum(weights[np.where(y == 1)])\n",
    "        if not total_neg_weights:\n",
    "            total_neg_weights = np.sum(weights[np.where(y == 0)])\n",
    "\n",
    "        # feature 값 기준으로 정렬\n",
    "        sorted_features = sorted(zip(weights, X, y), key=lambda a: a[1])\n",
    "\n",
    "        pos_seen, neg_seen = 0, 0\n",
    "        sum_pos_weights, sum_neg_weights = 0, 0\n",
    "        min_error, best_feature, best_threshold, best_polarity = float('inf'), None, None, None\n",
    "\n",
    "        for w, f, label in sorted_features:\n",
    "            # 오차 계산: 긍정/부정 가중치 중 작은 값 선택\n",
    "            error = min(\n",
    "                sum_neg_weights + (total_pos_weights - sum_pos_weights),\n",
    "                sum_pos_weights + (total_neg_weights - sum_neg_weights)\n",
    "            )\n",
    "\n",
    "            # 최소 오차인 경우 임계값과 polarity 업데이트\n",
    "            if error < min_error:\n",
    "                min_error = error\n",
    "                self.threshold = f\n",
    "                self.polarity = 1 if pos_seen > neg_seen else -1\n",
    "\n",
    "            # 레이블에 따라 가중치와 카운트 업데이트\n",
    "            if label == 1:\n",
    "                pos_seen += 1\n",
    "                sum_pos_weights += w\n",
    "            else:\n",
    "                neg_seen += 1\n",
    "                sum_neg_weights += w\n",
    "\n",
    "\n",
    "class AdaBoost:\n",
    "    def __init__(self, n_estimators=10):\n",
    "        # AdaBoost 초기화: n_estimators는 약한 분류기의 수\n",
    "        self.n_estimators = n_estimators\n",
    "        self.alphas = []  # 약한 분류기 가중치\n",
    "        self.clfs = []  # 약한 분류기 리스트\n",
    "\n",
    "    def train(self, X, y, features, X_ii):\n",
    "        # 긍정, 부정 샘플 수 계산\n",
    "        pos_num = np.sum(y)\n",
    "        neg_num = len(y) - pos_num\n",
    "        weights = np.zeros(len(y), dtype=np.float32)\n",
    "\n",
    "        # 초기 가중치 설정\n",
    "        for i in range(len(y)):\n",
    "            if y[i] == 1:  # 긍정 샘플\n",
    "                weights[i] = 1.0 / (pos_num * 2.0)\n",
    "            else:  # 부정 샘플\n",
    "                weights[i] = 1.0 / (neg_num * 2.0)\n",
    "\n",
    "        print(\"Training...\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        # 각 약한 분류기를 학습\n",
    "        for t in range(self.n_estimators):\n",
    "            print(f\"Training {t + 1} classifiers out of {self.n_estimators}\")\n",
    "\n",
    "            # 가중치 정규화\n",
    "            w_sum = np.sum(weights)\n",
    "            if w_sum == 0.0:\n",
    "                print(\"[WARNING] EARLY STOP. WEIGHTS ARE ZERO.\")\n",
    "                break\n",
    "            weights = weights / w_sum\n",
    "\n",
    "            # 약한 분류기 학습\n",
    "            print(\"Training weak classifiers...\")\n",
    "            weak_classifiers = self.train_estimators(X, y, weights, features)\n",
    "\n",
    "            # 가장 낮은 오차를 가진 약한 분류기 선택\n",
    "            print(\"Selecting best weak classifiers...\")\n",
    "            clf, error, incorrectness = self.select_best(weak_classifiers, X, y, weights)\n",
    "\n",
    "            if error <= 0.5:\n",
    "                # alpha와 beta 계산\n",
    "                beta = error / (1.0 - error)\n",
    "                alpha = math.log(1.0 / (beta + 1e-18))\n",
    "\n",
    "                # 가중치 업데이트\n",
    "                weights = np.multiply(weights, beta ** (1 - incorrectness))\n",
    "\n",
    "                # 최종 분류기 및 가중치 저장\n",
    "                self.alphas.append(alpha)\n",
    "                self.clfs.append(clf)\n",
    "            else:\n",
    "                print(error)\n",
    "                print(\"WHAT THE FUCK!????\")\n",
    "\n",
    "        print(f\"<== Training completed. Num. classifiers: {self.n_estimators}\")\n",
    "\n",
    "    def train_estimators(self, X, y, weights, features):\n",
    "        # 약한 분류기 학습\n",
    "        weak_clfs = []\n",
    "        total_pos_weights, total_neg_weights = 0, 0\n",
    "\n",
    "        for w, label in zip(weights, y):\n",
    "            if label == 1:\n",
    "                total_pos_weights += w\n",
    "            else:\n",
    "                total_neg_weights += w\n",
    "\n",
    "        for i in range(len(X)):\n",
    "            # 각 feature에 대해 약한 분류기 학습\n",
    "            clf = WeakClassifier(haar_feature=features[i])\n",
    "            clf.train(X[i], y, weights, total_pos_weights, total_neg_weights)\n",
    "            weak_clfs.append(clf)\n",
    "\n",
    "        return weak_clfs\n",
    "\n",
    "    def select_best(self, weak_clfs, X, y, weights):\n",
    "        # 최적의 약한 분류기를 선택\n",
    "        best_clf, min_error, best_accuracy = None, float('inf'), None\n",
    "\n",
    "        for i, clf in enumerate(weak_clfs):\n",
    "            # 오차 계산\n",
    "            incorrectness = np.abs(clf.classify_f(X[i]) - y)\n",
    "            error = float(np.sum(np.multiply(incorrectness, weights))) / len(incorrectness)\n",
    "\n",
    "            # 최소 오차를 가진 분류기 선택\n",
    "            if error < min_error:\n",
    "                best_clf, min_error, best_accuracy = clf, error, incorrectness\n",
    "\n",
    "        return best_clf, min_error, best_accuracy\n",
    "\n",
    "    def classify(self, X, scale=1.0):\n",
    "        # 최종 분류 수행: 여러 약한 분류기의 가중치를 반영한 결과 반환\n",
    "        total = sum(list(map(lambda x: x[0] * x[1].classify(X, scale), zip(self.alphas, self.clfs))))\n",
    "        return 1 if total >= 0.5 * sum(self.alphas) else 0\n",
    "\n",
    "\n",
    "\n",
    "class ViolaJones:\n",
    "\n",
    "    def __init__(self, layers, features_path=None):\n",
    "        assert isinstance(layers, list)\n",
    "        self.layers = layers  # list with the number T of weak classifiers\n",
    "        self.clfs = []\n",
    "        self.base_width, self.base_height = 19, 19  # Size of the images from training dataset\n",
    "        self.base_scale, self.shift = 1.25, 2\n",
    "        self.features_path = features_path  # Path to save the features\n",
    "\n",
    "    def train(self, X, y):\n",
    "        \"\"\"\n",
    "        We train N Viola-Jones classifiers (AdaBoost), each more complex than the previous ones.\n",
    "        After the first one, each classifier is trained with the positive examples plus\n",
    "        the false positives of the previous one.\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"Preparing data...\")\n",
    "\n",
    "        # Prepare training data\n",
    "        pos_num = np.sum(y)\n",
    "        neg_num = len(y) - pos_num\n",
    "        img_h, img_w = X[0].shape  # All training images must have the same size\n",
    "\n",
    "        # Split positives and negatives samples\n",
    "        pos_indices = np.where(y == 1)[0]\n",
    "        neg_indices = np.where(y == 0)[0]\n",
    "\n",
    "        # Show data info\n",
    "        print(\"Summary input data:\")\n",
    "        print(\"\\t- Total faces: {:,} ({:.2f}%)\".format(int(pos_num), 100.0 * pos_num / (pos_num + neg_num)))\n",
    "        print(\"\\t- Total non-faces: {:,} ({:.2f}%)\".format(int(neg_num), 100.0 * neg_num / (pos_num + neg_num)))\n",
    "        print(\"\\t- Total samples: {:,}\".format(int(pos_num + neg_num)))\n",
    "        print(\"\\t- Size (WxH): {}x{}\".format(img_w, img_h))\n",
    "\n",
    "        # Initialize weights and compute integral images\n",
    "        print(\"Generating integral images...\")\n",
    "        start_time = time.time()\n",
    "        X_ii = np.array(list(map(lambda x: integral_image(x), X)), dtype=np.uint32)\n",
    "        print(\"\\t- Num. integral images: {:,}\".format(len(X_ii)))\n",
    "\n",
    "        # Create and apply features\n",
    "        print(\"Building features...\")\n",
    "        start_time = time.time()\n",
    "        features = build_filters(img_w, img_h)  # Same features for all images\n",
    "        print(\"\\t- Num. features: {:,}\".format(len(features)))\n",
    "\n",
    "        print(\"Applying features...\")\n",
    "        start_time = time.time()\n",
    "        X_f = self.__load_feature_dataset()  # Load feature dataset (if exists)\n",
    "        if X_f is None:\n",
    "            X_f = apply_filters(X_ii, features)\n",
    "\n",
    "            if self.features_path:  # Save features\n",
    "                np.save(self.features_path + \"xf\" + \".npy\", X_f)\n",
    "                print(\"Applied features file saved!\")\n",
    "        print(\"\\t- Num. features applied: {:,}\".format(len(X_f) * len(features)))\n",
    "\n",
    "        # # Percentile optimization\n",
    "        # indices = SelectPercentile(f_classif, percentile=10).fit(X_f.T, y).get_support(indices=True)\n",
    "        # X_f = X_f[indices]\n",
    "        # features = np.array(features)[indices]\n",
    "\n",
    "        # Train cascade of Viola-Jones classifiers (AdaBoost)\n",
    "        for i, t in enumerate(self.layers):\n",
    "            print(\"[CascadeClassifier] Training {} of out {} layers\".format(i+1, len(self.layers)))\n",
    "            if len(neg_indices) == 0:\n",
    "                print('Early stop. All samples were correctly classify.')\n",
    "                break\n",
    "\n",
    "            # Merge indices and shuffle\n",
    "            tr_idxs = np.concatenate([pos_indices, neg_indices])\n",
    "            np.random.shuffle(tr_idxs)\n",
    "\n",
    "            # Train Viola-Jones (AdaBoost)\n",
    "            clf = AdaBoost(n_estimators=t)\n",
    "            clf.train(X_f[:, tr_idxs], y[tr_idxs], features, X_ii[tr_idxs])\n",
    "            self.clfs.append(clf)\n",
    "\n",
    "            # Find which non-faces where label as a face\n",
    "            false_positives = []\n",
    "            for neg_idx in neg_indices:\n",
    "                if self.classify(X[neg_idx]) == 1:\n",
    "                    false_positives.append(neg_idx)\n",
    "            neg_indices = np.array(false_positives)\n",
    "\n",
    "    def classify(self, image, scale=1.0):\n",
    "        \"\"\"\n",
    "        If a no-face is found, reject now. Else, keep looking.\n",
    "        \"\"\"\n",
    "        return self.classify_ii(integral_image(image), scale)\n",
    "\n",
    "    def classify_ii(self, ii, scale=1.0):\n",
    "        \"\"\"\n",
    "        If a no-face is found, reject now. Else, keep looking.\n",
    "        \"\"\"\n",
    "        for clf in self.clfs:  # ViolaJones\n",
    "            if clf.classify(ii, scale) == 0:\n",
    "                return 0\n",
    "        return 1\n",
    "\n",
    "    def find_faces(self, pil_image):\n",
    "        \"\"\"\n",
    "        Receives a PIL image\n",
    "        \"\"\"\n",
    "        w, h, s = (self.base_width, self.base_height, self.base_scale)\n",
    "        regions = []\n",
    "\n",
    "        # Preprocess image\n",
    "        pil_image = pil_image.convert('L')\n",
    "        image = np.array(pil_image)\n",
    "        img_h, img_w = image.shape\n",
    "\n",
    "        # Compute integral image\n",
    "        ii = integral_image(image)\n",
    "\n",
    "        # Sliding window\n",
    "        # Box must be smaller than the image\n",
    "        counter = 0\n",
    "        while int(w*s) < img_w and int(h*s) < img_h:\n",
    "\n",
    "            # The box must slide just in the image\n",
    "            for y1 in np.arange(0, int(img_h)-int(h*s), self.shift):\n",
    "                for x1 in np.arange(0, int(img_w)-int(w*s), self.shift):\n",
    "                    y1, x1 = int(y1), int(x1)\n",
    "                    y2, x2 = y1 + int(h*s), x1 + int(w*s)\n",
    "                    cropped_img = ii[y1:y2, x1:x2]\n",
    "\n",
    "                    if self.classify_ii(cropped_img, scale=s):  # CascadeClassifier\n",
    "                        regions.append((x1, y1, x2, y2))\n",
    "\n",
    "                    counter += 1\n",
    "                    print(\"Crops analized: {}\".format(counter))\n",
    "\n",
    "            # Increase scale of the window\n",
    "            w *= s\n",
    "            h *= s\n",
    "\n",
    "        return regions\n",
    "\n",
    "    def save(self, filename):\n",
    "        with open(filename + \".pkl\", 'wb') as f:\n",
    "            pickle.dump(self, f)\n",
    "\n",
    "    @staticmethod\n",
    "    def load(filename):\n",
    "        with open(filename, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "    def __load_feature_dataset(self):\n",
    "        X_f = None\n",
    "        # Load precomputed features\n",
    "        try:\n",
    "            if self.features_path:\n",
    "                X_f = np.load(self.features_path + \"xf\" + \".npy\")\n",
    "                print(\"Precomputed dataset loaded!\")\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "        return X_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Viola-Jones...\n",
      "Preparing data...\n",
      "Summary input data:\n",
      "\t- Total faces: 2,429 (34.81%)\n",
      "\t- Total non-faces: 4,548 (65.19%)\n",
      "\t- Total samples: 6,977\n",
      "\t- Size (WxH): 19x19\n",
      "Generating integral images...\n",
      "\t- Num. integral images: 6,977\n",
      "Building features...\n",
      "\t- Num. features: 11,376\n",
      "Applying features...\n",
      "Applied features file saved!\n",
      "\t- Num. features applied: 129,413,376\n",
      "[CascadeClassifier] Training 1 of out 4 layers\n",
      "Training...\n",
      "Training 1 classifiers out of 1\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "<== Training completed. Num. classifiers: 1\n",
      "[CascadeClassifier] Training 2 of out 4 layers\n",
      "Training...\n",
      "Training 1 classifiers out of 10\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 2 classifiers out of 10\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 3 classifiers out of 10\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 4 classifiers out of 10\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 5 classifiers out of 10\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 6 classifiers out of 10\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 7 classifiers out of 10\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 8 classifiers out of 10\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 9 classifiers out of 10\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 10 classifiers out of 10\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "<== Training completed. Num. classifiers: 10\n",
      "[CascadeClassifier] Training 3 of out 4 layers\n",
      "Training...\n",
      "Training 1 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 2 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 3 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 4 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 5 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 6 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 7 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 8 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 9 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 10 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 11 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 12 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 13 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 14 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 15 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 16 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 17 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 18 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 19 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 20 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 21 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 22 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 23 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 24 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 25 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 26 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 27 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 28 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 29 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 30 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 31 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 32 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 33 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 34 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 35 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 36 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 37 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 38 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 39 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 40 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 41 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 42 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 43 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 44 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 45 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 46 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 47 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 48 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 49 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 50 classifiers out of 50\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "<== Training completed. Num. classifiers: 50\n",
      "[CascadeClassifier] Training 4 of out 4 layers\n",
      "Training...\n",
      "Training 1 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 2 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 3 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 4 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 5 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 6 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 7 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 8 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 9 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 10 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 11 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 12 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 13 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 14 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 15 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 16 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 17 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 18 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 19 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 20 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 21 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 22 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 23 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 24 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 25 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 26 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 27 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 28 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 29 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 30 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 31 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 32 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 33 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 34 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 35 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 36 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 37 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 38 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 39 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 40 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 41 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 42 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 43 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 44 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 45 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 46 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 47 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 48 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 49 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 50 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 51 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 52 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 53 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 54 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 55 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 56 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 57 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 58 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 59 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 60 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 61 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 62 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 63 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 64 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 65 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 66 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 67 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 68 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 69 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 70 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 71 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 72 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 73 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 74 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 75 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 76 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 77 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 78 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 79 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 80 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 81 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 82 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 83 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 84 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 85 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 86 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 87 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 88 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 89 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 90 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 91 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 92 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 93 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 94 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 95 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 96 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 97 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 98 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 99 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "Training 100 classifiers out of 100\n",
      "Training weak classifiers...\n",
      "Selecting best weak classifiers...\n",
      "<== Training completed. Num. classifiers: 100\n",
      "Training finished!\n",
      "\n",
      "Saving weights...\n",
      "Weights saved!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTraining Viola-Jones...\")\n",
    "features_path = '/workspace/weights'\n",
    "clf = ViolaJones(layers=[1, 10, 50, 100], features_path=features_path)\n",
    "clf.train(X, y)  # X_f (optional, to speed-up training)\n",
    "print(\"Training finished!\")\n",
    "\n",
    "# Save weights\n",
    "print(\"\\nSaving weights...\")\n",
    "clf.save(features_path + 'cvj_weights_' + str(int(time.time())))\n",
    "print(\"Weights saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_folder_path = '/workspace/mlcnn/mitcbcl/test/face'\n",
    "non_face_folder_path = '/workspace/mlcnn/mitcbcl/test/non-face'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "face_images: (472, 19, 19)\n",
      "non_face_images: (23573, 19, 19)\n"
     ]
    }
   ],
   "source": [
    "face_images = load_all_pgms(face_folder_path)\n",
    "non_face_images = load_all_pgms(non_face_folder_path)\n",
    "\n",
    "print(f\"face_images: {face_images.shape}\")\n",
    "print(f\"non_face_images: {non_face_images.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (24045, 19, 19)\n",
      "y: (24045,)\n"
     ]
    }
   ],
   "source": [
    "X = np.concatenate([face_images, non_face_images], axis=0)\n",
    "y = np.zeros(len(face_images) + len(non_face_images))\n",
    "y[:len(face_images)] = 1\n",
    "\n",
    "print(f\"X: {X.shape}\")\n",
    "print(f\"y: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(clf, X, y, show_samples=False):\n",
    "    metrics = {}\n",
    "    true_positive, true_negative = 0, 0  # Correct\n",
    "    false_positive, false_negative = 0, 0  # Incorrect\n",
    "\n",
    "    for i in range(len(y)):\n",
    "        prediction = clf.classify(X[i])\n",
    "        if prediction == y[i]:  # Correct\n",
    "            if prediction == 1:  # Face\n",
    "                true_positive += 1\n",
    "            else:  # No-face\n",
    "                true_negative += 1\n",
    "        else:  # Incorrect\n",
    "            #if show_samples: show_sample(X[i], y[i], prediction)\n",
    "\n",
    "            if prediction == 1:  # Face\n",
    "                false_positive += 1\n",
    "            else:  # No-face\n",
    "                false_negative += 1\n",
    "\n",
    "    # Compute metrics\n",
    "    metrics['true_positive'] = true_positive\n",
    "    metrics['true_negative'] = true_negative\n",
    "    metrics['false_positive'] = false_positive\n",
    "    metrics['false_negative'] = false_negative\n",
    "\n",
    "    metrics['accuracy'] = (true_positive + true_negative)/(true_positive+false_negative+true_negative+false_positive)\n",
    "    metrics['precision'] = true_positive / (true_positive+false_positive)\n",
    "    metrics['recall'] = true_positive / (true_positive+false_negative)  # or Sensitivity\n",
    "    metrics['specifity'] = true_negative/(true_negative+false_positive)\n",
    "    metrics['f1'] = (2.0 * metrics['precision'] * metrics['recall']) / (metrics['precision'] + metrics['recall'])\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating...\n",
      "Metrics: [yenna]\n",
      "\t- true_positive: 116\n",
      "\t- true_negative: 23,008\n",
      "\t- false_positive: 565\n",
      "\t- false_negative: 356\n",
      "\t- accuracy: 0.962\n",
      "\t- precision: 0.170\n",
      "\t- recall: 0.246\n",
      "\t- specifity: 0.976\n",
      "\t- f1: 0.201\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "name = \"yenna\"\n",
    "print(\"\\nEvaluating...\")\n",
    "metrics = evaluate(clf, X, y, show_samples=False)\n",
    "\n",
    "print(\"Metrics: [{}]\".format(name))\n",
    "counter = 0\n",
    "for k, v in metrics.items():\n",
    "    counter += 1\n",
    "    if counter <= 4:\n",
    "        print(\"\\t- {}: {:,}\".format(k, v))\n",
    "    else:\n",
    "        print(\"\\t- {}: {:.3f}\".format(k, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw\n",
    "from nms import nms\n",
    "\n",
    "\n",
    "def draw_bounding_boxes(pil_image, regions, color=\"green\", thickness=3):\n",
    "    # Prepare image\n",
    "    source_img = pil_image.convert(\"RGBA\")\n",
    "    draw = ImageDraw.Draw(source_img)\n",
    "    for rect in regions:\n",
    "        draw.rectangle(tuple(rect), outline=color, width=thickness)\n",
    "    return source_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_faces():\n",
    "    weight_path = \"/workspace/weightscvj_weights_1728987914.pkl\"\n",
    "    face_path = \"/workspace/mlcnn/mitcbcl/train/face/face00067.pgm\"\n",
    "\n",
    "\n",
    "    # Load classifier weights\n",
    "    clf = ViolaJones.load(weight_path)\n",
    "\n",
    "    # Find regions of the faces\n",
    "    numpy_img = load_pgm_image(face_path)\n",
    "    pil_img = Image.fromarray(numpy_img)\n",
    "    regions = clf.find_faces(pil_img)\n",
    "\n",
    "    # Draw bouding boxes\n",
    "    # TODO: Review Non-maximum supression (fix own implementation)\n",
    "    # regions = np.array([(10, 10, 50, 50), (20, 20, 60, 60), (37, 59, 199, 244), (47, 69, 209, 254)])\n",
    "    scores = [1.0]*len(regions)  #np.ones(len(regions))\n",
    "    indicies = nms.boxes(regions, scores)\n",
    "    regions = np.array(regions)\n",
    "    print(f\"scores: {scores}\")\n",
    "    print(f\"regions: {regions}\")\n",
    "\n",
    "    drawn_img = draw_bounding_boxes(pil_img, list(regions[indicies]), thickness=1)\n",
    "    # drawn_img = draw_bounding_boxes(pil_img, list(regions), thickness=1)\n",
    "\n",
    "    # Show image\n",
    "    plt.imshow(drawn_img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores: []\n",
      "regions: []\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "find_faces()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
